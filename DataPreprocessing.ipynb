{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013df9ef-995a-44a3-91ac-405386c74977",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c100619f-3b34-4821-afbc-83626470c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 3 articles.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_articles_from_folder(folder_path):\n",
    "    \"\"\"Load all .txt files from the given folder and return them as a list of texts\"\"\"\n",
    "    texts = []\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_names.append(file_name)\n",
    "            with open(os.path.join(folder_path, file_name), \"r\", encoding=\"utf-8\") as file:\n",
    "                texts.append(file.read())\n",
    "    return file_names, texts\n",
    "\n",
    "# Set the folder path where your articles are stored\n",
    "folder_path = \"articles/\"  # Place your .txt files in this folder\n",
    "file_names, articles = load_articles_from_folder(folder_path)\n",
    "\n",
    "print(f\" Loaded {len(articles)} articles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6fa60-51e7-477a-ba91-3c36702afd57",
   "metadata": {},
   "source": [
    "### Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63eb6480-6419-4fe8-a913-2c0c4baa625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Text preprocessing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\radhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces and newlines\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?;:\\'\\s]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to all articles\n",
    "cleaned_articles = [preprocess_text(article) for article in articles]\n",
    "\n",
    "print(\" Text preprocessing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a6bb8-a04c-42fd-a6a1-132671232948",
   "metadata": {},
   "source": [
    "### Chunk the Text \n",
    "Since the articles may be too large, we need to split them into 200-300 word chunks for better retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3308d694-cd46-4d26-bb98-e400aaacb705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated 29 text chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(texts, chunk_size=500, overlap=100):\n",
    "    \"\"\"Chunk text into smaller parts\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=overlap\n",
    "    )\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        chunks.extend(text_splitter.split_text(text))\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking\n",
    "text_chunks = chunk_text(cleaned_articles)\n",
    "\n",
    "print(f\" Generated {len(text_chunks)} text chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f85bdcc7-bd4d-4c29-8d29-9bdbde6bf767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save text chunks to a file\n",
    "with open(\"preprocessed_text_chunks.pkl\", \"wb\") as file:\n",
    "    pickle.dump(text_chunks, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a97fa6-996e-4ed1-bddc-dd8b487484b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
